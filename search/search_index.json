{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"FakeArtDetector","text":"<p>An MLOps project for classifying images as Real vs AI-generated using the CIFAKE dataset.</p>"},{"location":"#what-this-project-does","title":"What this project does","text":"<ul> <li>Preprocesses CIFAKE into PyTorch tensors in <code>data/processed/</code></li> <li>Trains a small CNN and saves a checkpoint to <code>models/model.pth</code></li> <li>Evaluates the checkpoint on the CIFAKE test split</li> <li>Produces simple figures (training curves and embedding visualization)</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<pre><code># Install dependencies\nuv sync\n\n# Download/process CIFAKE into data/processed/\nuv run invoke preprocess-data\n\n# Train and write a checkpoint to models/model.pth\nuv run invoke train\n\n# Evaluate the checkpoint on the test split\nuv run invoke evaluate\n\n# Visualize embeddings (writes a figure to reports/figures/)\nuv run invoke visualize\n</code></pre>"},{"location":"#key-outputs","title":"Key outputs","text":"<ul> <li><code>data/processed/train_images.pt</code></li> <li><code>data/processed/train_target.pt</code></li> <li><code>data/processed/test_images.pt</code></li> <li><code>data/processed/test_target.pt</code></li> <li><code>models/model.pth</code></li> <li><code>reports/figures/training_statistics.png</code></li> <li><code>reports/figures/embeddings.png</code> (default)</li> </ul>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Setup and local run instructions: Getting Started</li> <li>Day-to-day commands and artifacts: Workflows</li> <li>API reference: see the API Reference section in the navigation</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python $\\ge$ 3.11</li> <li><code>uv</code> (recommended) or another environment manager</li> </ul>"},{"location":"getting-started/#install","title":"Install","text":"<p>From the repository root:</p> <pre><code>uv sync\n</code></pre>"},{"location":"getting-started/#run-the-common-tasks","title":"Run the common tasks","text":"<p>This project uses <code>invoke</code> tasks defined in <code>tasks.py</code>.</p> <pre><code># Run unit tests\nuv run invoke test\n\n# Preprocess/download dataset into data/processed/\nuv run invoke preprocess-data\n\n# Train\nuv run invoke train\n\n# See training options\nuv run invoke train --help\n\n# List available Hydra config groups\nuv run invoke list-configs\n\n# Evaluate (uses cfg.evaluate.model_checkpoint by default)\nuv run invoke evaluate\n\n# See evaluation options\nuv run invoke evaluate --help\n</code></pre>"},{"location":"getting-started/#build-or-serve-the-docs","title":"Build or serve the docs","text":"<pre><code># Build static site into ./build/\nuv run invoke build-docs\n\n# Serve locally (hot reload)\nuv run invoke serve-docs\n</code></pre>"},{"location":"workflows/","title":"Workflows","text":"<p>This page collects the common end-to-end commands for running the project locally.</p>"},{"location":"workflows/#data-preprocessing","title":"Data preprocessing","text":"<p>Preprocessing downloads CIFAKE from Hugging Face, converts images to tensors, and saves them to <code>data/processed/</code>.</p> <p>Run via Invoke (recommended):</p> <pre><code>uv run invoke preprocess-data\n</code></pre> <p>Outputs:</p> <ul> <li><code>data/processed/train_images.pt</code></li> <li><code>data/processed/train_target.pt</code></li> <li><code>data/processed/test_images.pt</code></li> <li><code>data/processed/test_target.pt</code></li> </ul>"},{"location":"workflows/#training","title":"Training","text":"<p>Training loads the processed tensors, trains <code>FakeArtClassifier</code>, saves a checkpoint, and writes a simple training figure.</p> <p>Run via Invoke:</p> <pre><code>uv run invoke train\n</code></pre> <p>Outputs:</p> <ul> <li><code>models/model.pth</code></li> <li><code>reports/figures/training_statistics.png</code></li> </ul>"},{"location":"workflows/#evaluation","title":"Evaluation","text":"<p>Evaluation loads <code>models/model.pth</code> (a PyTorch <code>state_dict</code>), runs inference on the CIFAKE test split, and prints test accuracy.</p> <p>Run via Invoke:</p> <pre><code>uv run invoke evaluate\n</code></pre>"},{"location":"workflows/#visualization","title":"Visualization","text":"<p>Visualization computes embeddings from the trained model on the CIFAKE test split, reduces them to 2D (t-SNE), and saves a scatter plot.</p> <p>Run via Invoke:</p> <pre><code>uv run invoke visualize\n</code></pre> <p>Outputs (default):</p> <ul> <li><code>reports/figures/embeddings.png</code></li> </ul>"},{"location":"workflows/#tests","title":"Tests","text":"<p>Run the test suite with coverage:</p> <pre><code>uv run invoke test\n</code></pre>"},{"location":"workflows/#docker-images","title":"Docker images","text":"<p>Build the Docker images via Invoke:</p> <pre><code>uv run invoke docker-build\n</code></pre> <p>Or run Docker directly:</p> <pre><code>docker build -t train:latest . -f dockerfiles/train.dockerfile\ndocker build -t api:latest . -f dockerfiles/api.dockerfile\n</code></pre>"},{"location":"workflows/#docs","title":"Docs","text":"<p>Serve docs locally:</p> <pre><code>uv run invoke serve-docs\n</code></pre> <p>Build docs into <code>./build/</code>:</p> <pre><code>uv run invoke build-docs\n</code></pre>"},{"location":"reference/api/","title":"<code>Api</code>","text":""},{"location":"reference/api/#fakeartdetector.api","title":"fakeartdetector.api","text":""},{"location":"reference/api/#fakeartdetector.api.add_to_database","title":"add_to_database","text":"<pre><code>add_to_database(latency: float, embedding: float, prediction: float) -&gt; None\n</code></pre> <p>Save input image and prediction to database.</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>def add_to_database(\n    latency: float,\n    embedding: float,\n    prediction: float,\n) -&gt; None:\n    \"\"\"Save input image and prediction to database.\"\"\"\n    try:\n        row_id = insert_row(latency, embedding, prediction)\n        print(f\"Inserted row into sqlite db id={row_id}\")\n    except Exception as e:\n        print(f\"Error inserting row into sqlite db: {e}\")\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.download_db","title":"download_db","text":"<pre><code>download_db()\n</code></pre> <p>Returns the sqlite database file used for inference logs. Example Usage:     curl -X GET 'http://localhost:8000/download-db' --output ./dbfile.db</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.get(\"/download-db\")\ndef download_db():\n    \"\"\"Returns the sqlite database file used for inference logs.\n    Example Usage:\n        curl -X GET 'http://localhost:8000/download-db' --output ./dbfile.db\n    \"\"\"\n    db_path = os.getenv(\"SQLITE_DB_PATH\", \"data/inference_logs/inference_logs.db\")\n    if not os.path.exists(db_path):\n        return {\"error\": \"Database file not found\", \"path\": db_path}\n    return FileResponse(db_path, media_type=\"application/x-sqlite3\", filename=os.path.basename(db_path))\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.download_model","title":"download_model","text":"<pre><code>download_model(bucket_name, source_blob_name, destination_file_name)\n</code></pre> <p>Downloads a blob from the bucket.</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>def download_model(bucket_name, source_blob_name, destination_file_name):\n    \"\"\"Downloads a blob from the bucket.\"\"\"\n    storage_client = storage.Client()\n    bucket = storage_client.bucket(bucket_name)\n    blob = bucket.blob(source_blob_name)\n    blob.download_to_filename(destination_file_name)\n    print(f\"Downloaded {source_blob_name} to {destination_file_name}.\")\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.get_wandb_models","title":"get_wandb_models","text":"<pre><code>get_wandb_models(limit_collections: int = Query(5, ge=1, le=50, description='Max number of collections to scan'), latest_per_collection: bool = Query(True, description='Return only the newest version per collection'), per_collection: int = Query(1, ge=1, le=20, description='Max artifacts per collection if not latest_per_collection'), limit_models: int = Query(10, ge=1, le=200, description='Max number of models returned'))\n</code></pre> <p>Returns available models from wandb registry</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.get(\"/wandb-models\")\ndef get_wandb_models(\n    limit_collections: int = Query(5, ge=1, le=50, description=\"Max number of collections to scan\"),\n    latest_per_collection: bool = Query(True, description=\"Return only the newest version per collection\"),\n    per_collection: int = Query(\n        1, ge=1, le=20, description=\"Max artifacts per collection if not latest_per_collection\"\n    ),\n    limit_models: int = Query(10, ge=1, le=200, description=\"Max number of models returned\"),\n):\n    \"\"\"Returns available models from wandb registry\"\"\"\n    try:\n        api = wandb.Api(api_key=os.getenv(\"WANDB_API_KEY\"))\n        entity = os.getenv(\"WANDB_ENTITY\")\n        project = os.getenv(\"WANDB_PROJECT\")\n\n        if not entity or not project:\n            return {\"error\": \"WANDB_ENTITY and WANDB_PROJECT must be set\", \"models\": []}\n\n        collections = api.artifact_collections(type_name=\"model\", project_name=f\"{entity}/{project}\", per_page=5)\n\n        collections = list(collections)\n        total_collections = len(collections)\n        print(f\"Total collections found: {total_collections}\")\n\n        # Keep this endpoint fast: only scan a small number of collections.\n        collections = collections[:limit_collections]\n\n        models = []\n\n        for coll in collections:\n            # The W&amp;B SDK may support pagination args; fall back gracefully.\n            try:\n                artifact_iter = coll.artifacts(per_page=1 if latest_per_collection else per_collection)\n            except TypeError:\n                artifact_iter = coll.artifacts()\n\n            taken = 0\n            for art in artifact_iter:\n                models.append(\n                    {\n                        \"collection_name\": coll.name,\n                        \"version\": art.version,\n                        \"full_path\": f\"{art.entity}/{art.project}/{coll.name}:{art.version}\",\n                        \"created_at\": (\n                            art.created_at\n                            if isinstance(art.created_at, str)\n                            else art.created_at.isoformat()\n                            if art.created_at\n                            else None\n                        ),\n                        \"aliases\": list(art.aliases) if art.aliases else [],\n                    }\n                )\n                taken += 1\n                if latest_per_collection:\n                    break\n                if taken &gt;= per_collection:\n                    break\n\n        # Sort newest first (ISO strings sort correctly)\n        models.sort(key=lambda x: x[\"created_at\"] or \"\", reverse=True)\n\n        models = models[:limit_models]\n\n        print(f\"Returning {len(models)} models from wandb\")\n        return {\n            \"total_collections\": total_collections,\n            \"scanned_collections\": len(collections),\n            \"returned_models\": len(models),\n            \"models\": models,\n        }\n\n    except Exception as e:\n        print(f\"Error fetching wandb models: {e}\")\n        return {\"error\": str(e), \"models\": []}\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.int_item","title":"int_item","text":"<pre><code>int_item()\n</code></pre> <p>http://127.0.0.1:8000/models</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.get(\"/models\")\ndef int_item():\n    \"\"\"http://127.0.0.1:8000/models\"\"\"\n    model_list = os.listdir(\"./models\")\n    return model_list\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.lifespan","title":"lifespan  <code>async</code>","text":"<pre><code>lifespan(app: FastAPI)\n</code></pre> <p>Loads the model and gets ready for inference</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Loads the model and gets ready for inference\"\"\"\n    # Ensure directories exist\n    load_dotenv()\n    global model, DEVICE, transform, loaded_model_source, api\n    print(\"Hello, i am loading the model\")\n    DEVICE = device(\"cuda\" if cuda.is_available() else \"mps\" if mps.is_available() else \"cpu\")\n    print(f\"Using device: {DEVICE}\")\n    # Initialize sqlite database for inference logs\n    db_path = os.getenv(\"SQLITE_DB_PATH\", \"data/inference_logs/inference_logs.db\")\n    try:\n        init_db(db_path)\n        print(f\"Initialized sqlite DB at: {db_path}\")\n    except Exception as e:\n        print(f\"Failed to initialize sqlite DB at {db_path}: {e}\")\n\n    model = FakeArtClassifier().to(DEVICE)\n    loaded_model_source = \"None\"\n\n    # Priority order:\n    # 1. USE_LOCAL flag (local model path)\n    # 2. MODEL_NAME env variable (wandb model)\n    # 3. LOAD_FROM_BUCKET flag (GCS model)\n    # 4. Fallback to local base_model.pth\n\n    if os.getenv(\"USE_LOCAL\"):\n        print(\"Loading model from local path...\")\n        local_model_path = \"models/base_model.pth\"\n        if os.path.exists(local_model_path):\n            state_dict = load(local_model_path, map_location=DEVICE)\n            model.load_state_dict(state_dict)\n            loaded_model_source = \"Local base_model.pth\"\n            print(\"Successfully loaded local model\")\n        else:\n            print(f\"Local model not found at {local_model_path}\")\n\n    elif os.getenv(\"MODEL_NAME\"):\n        print(\"Loading model from wandb MODEL_NAME...\")\n        model = load_model_wandb(os.getenv(\"MODEL_NAME\")).to(DEVICE)\n        loaded_model_source = os.getenv(\"MODEL_NAME\")\n\n    elif os.getenv(\"LOAD_FROM_BUCKET\") in (\"true\", \"1\", \"yes\"):\n        bucket_name = os.environ.get(\"GCS_BUCKET_NAME\")\n        model_file = os.environ.get(\"MODEL_FILE\")\n        local_model_path = \"model.pth\"\n\n        if bucket_name and model_file:\n            try:\n                print(f\"Loading model from GCS: gs://{bucket_name}/{model_file}\")\n                download_model(bucket_name, model_file, local_model_path)\n                state_dict = load(local_model_path, map_location=DEVICE)\n                model.load_state_dict(state_dict)\n                loaded_model_source = f\"gs://{bucket_name}/{model_file}\"\n                print(\"Successfully loaded model from GCS\")\n            except Exception as e:\n                print(f\"Failed to load from GCS: {e}. Falling back to local model.\")\n\n    # Final fallback to base model\n    if loaded_model_source == \"None\":\n        print(\"Using fallback local base_model.pth\")\n        if os.path.exists(\"./models/base_model.pth\"):\n            state_dict = load(\"./models/base_model.pth\", map_location=DEVICE)\n            model.load_state_dict(state_dict)\n            loaded_model_source = \"Local base_model.pth\"\n        else:\n            print(\"No model found! Using random weights.\")\n            loaded_model_source = \"Random Weights (No model found)\"\n\n    transform = T.Compose(\n        [\n            T.Resize((32, 32)),\n            T.ToTensor(),\n        ]\n    )\n\n    print(f\"Model loaded from: {loaded_model_source}\")\n    print(f\"Model: {model}\\n\")\n    print(\"Model ready for inference\\n\")\n\n    yield\n    print(\"Cleaning up...\")\n    del model, DEVICE\n    print(\"Goodbye\")\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.list_inference_log_files","title":"list_inference_log_files","text":"<pre><code>list_inference_log_files()\n</code></pre> <p>List files in the inference logs directory with sizes (bytes and human readable).</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.get(\"/inference-logs/files\")\ndef list_inference_log_files():\n    \"\"\"List files in the inference logs directory with sizes (bytes and human readable).\"\"\"\n    db_path = os.getenv(\"SQLITE_DB_PATH\", \"data/inference_logs/inference_logs.db\")\n    dir_path = os.path.dirname(db_path) or \".\"\n\n    if not os.path.isdir(dir_path):\n        return {\"directory\": dir_path, \"files\": []}\n\n    def _human_size(n: int) -&gt; str:\n        for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n            if n &lt; 1024.0:\n                return f\"{n:.1f}{unit}\"\n            n /= 1024.0\n        return f\"{n:.1f}PB\"\n\n    files = []\n    for name in sorted(os.listdir(dir_path)):\n        full = os.path.join(dir_path, name)\n        if os.path.isfile(full):\n            try:\n                size = os.path.getsize(full)\n            except OSError:\n                size = None\n            files.append(\n                {\n                    \"name\": name,\n                    \"path\": full,\n                    \"size_bytes\": size,\n                    \"size_human\": _human_size(size) if isinstance(size, int) else None,\n                }\n            )\n\n    return {\"directory\": dir_path, \"files\": files}\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.model_inference","title":"model_inference  <code>async</code>","text":"<pre><code>model_inference(data: UploadFile = File(...), background_tasks: BackgroundTasks = BackgroundTasks())\n</code></pre> <p>Answers if the Image Sent is AI art of not</p> Example Usage <p>curl -X POST \"http://localhost:8000/model/\"             -F \"data=@cat.jpg\"</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.post(\"/model/\")\nasync def model_inference(data: UploadFile = File(...), background_tasks: BackgroundTasks = BackgroundTasks()):\n    \"\"\"Answers if the Image Sent is AI art of not\n\n    Example Usage:\n        curl -X POST \"http://localhost:8000/model/\" \\\n            -F \"data=@cat.jpg\" \\\n        \"\"\"\n\n    image_bytes = await data.read()\n    img = await image_clean_utility(image_bytes)\n    with request_latency.time():\n        with no_grad():\n            embeddings = model.classifier(model.backbone(img))\n            logits: Tensor = model.head(embeddings)\n            prob = sigmoid(logits)\n            is_ai = (prob &gt; 0.5).item()\n\n    background_tasks.add_task(\n        add_to_database,\n        latency=time.time(),\n        embedding=embeddings.cpu().numpy(),\n        prediction=prob.item(),\n    )\n    return {\"isAI\": is_ai, \"probability\": prob.item()}\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.model_info","title":"model_info","text":"<pre><code>model_info()\n</code></pre> <p>Returns model and device information</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.get(\"/model-info\")\ndef model_info():\n    \"\"\"Returns model and device information\"\"\"\n    return {\n        \"device\": str(DEVICE),\n        \"model\": str(model),\n        \"model_device\": str(next(model.parameters()).device),\n        \"loaded_model_source\": loaded_model_source,\n    }\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.root","title":"root","text":"<pre><code>root()\n</code></pre> <p>Health Check.</p> Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.get(\"/\")\ndef root():\n    \"\"\"Health Check.\"\"\"\n    return {\"message\": HTTPStatus.OK.phrase, \"status-code\": HTTPStatus.OK}\n</code></pre>"},{"location":"reference/api/#fakeartdetector.api.switch_model","title":"switch_model","text":"<pre><code>switch_model(model_path: str)\n</code></pre> <p>Switch to a different model from wandb registry</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Full wandb artifact path (e.g., 'entity/project/model-name:version')</p> required Source code in <code>src/fakeartdetector/api.py</code> <pre><code>@app.post(\"/switch-model\")\ndef switch_model(model_path: str):\n    \"\"\"Switch to a different model from wandb registry\n\n    Args:\n        model_path: Full wandb artifact path (e.g., 'entity/project/model-name:version')\n    \"\"\"\n    global model, loaded_model_source\n    try:\n        model = load_model_wandb(model_path).to(DEVICE)\n        loaded_model_source = model_path\n        return {\n            \"success\": True,\n            \"message\": f\"Successfully switched to model: {model_path}\",\n            \"loaded_model_source\": loaded_model_source,\n        }\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}\n</code></pre>"},{"location":"reference/configs/","title":"<code>Configs</code>","text":"<p>This project uses a <code>configs/</code> directory to hold YAML configuration files used by training, evaluation, and experiments.</p>"},{"location":"reference/configs/#overview","title":"Overview","text":"<ul> <li><code>configs/default_config.yaml</code>: base configuration values.</li> <li><code>configs/dataset/</code>: dataset-specific configuration (e.g., <code>base.yaml</code>, <code>cloud.yaml</code>).</li> <li><code>configs/experiment/</code>: experiment specifications and multi-trial definitions.</li> <li><code>configs/evaluate/</code>, <code>configs/logging/</code>, <code>configs/optimizer/</code>, <code>configs/profiler/</code>: modular configs for their respective subsystems.</li> </ul>"},{"location":"reference/configs/#how-to-use","title":"How to use","text":"<ul> <li>The training/evaluation scripts pick up the correct config via Hydra or direct parsing. See <code>src/fakeartdetector/train.py</code> and <code>src/fakeartdetector/evaluate.py</code> for how configs are loaded.</li> <li>To run an experiment locally, edit or override values with CLI flags as supported by the scripts.</li> </ul>"},{"location":"reference/configs/#best-practices","title":"Best practices","text":"<ul> <li>Keep sensitive values (API keys, GCS bucket names) in environment variables or a <code>.env</code> file and avoid committing them to the repo.</li> <li>Use the <code>configs/experiment/trials</code> files to run sweep experiments.</li> </ul>"},{"location":"reference/configs/#example-override","title":"Example (override)","text":"<pre><code>python src/fakeartdetector/train.py --config configs/default_config.yaml\n</code></pre> <p>You can also run the same behaviour via the project's Invoke task (recommended for consistent <code>uv</code> usage):</p> <pre><code>uv sync\nuv sync --dev\nuv run invoke train --config-name default_config.yaml\n</code></pre> <p>Note: <code>--config-name</code> refers to the top-level YAML filename (not a path). The referenced file lives at configs/default_config.yaml in the repository.</p>"},{"location":"reference/data/","title":"<code>Data</code>","text":"<p>This module contains the data preprocessing entrypoint for the CIFAKE dataset and small utilities for loading/visualizing the processed tensors.</p>"},{"location":"reference/data/#what-this-module-does","title":"What this module does","text":"<ul> <li>Downloads the CIFAKE dataset from Hugging Face</li> <li>Converts images to PyTorch tensors</li> <li>Saves processed tensors into <code>data/processed/</code> for later training/evaluation</li> </ul>"},{"location":"reference/data/#preprocess-the-dataset","title":"Preprocess the dataset","text":"<p>The preprocessing step writes four files:</p> <ul> <li><code>data/processed/train_images.pt</code></li> <li><code>data/processed/train_target.pt</code></li> <li><code>data/processed/test_images.pt</code></li> <li><code>data/processed/test_target.pt</code></li> </ul> <p>Run via the Invoke task (recommended):</p> <pre><code>uv run invoke preprocess-data\n</code></pre> <p>Or run the module directly:</p> <pre><code>uv run src/fakeartdetector/data.py data/processed\n</code></pre> <p>Notes:</p> <ul> <li>Images are resized to $32\\times32$ and converted to RGB.</li> <li>The preprocessing step currently does not apply normalization; it saves raw <code>ToTensor()</code> outputs in $[0, 1]$.</li> </ul>"},{"location":"reference/data/#load-processed-tensors","title":"Load processed tensors","text":"<p>Use <code>cifake()</code> to load the processed <code>.pt</code> files into <code>TensorDataset</code>s:</p> <pre><code>from fakeartdetector.data import cifake\n\ntrain_set, test_set = cifake()\n</code></pre>"},{"location":"reference/data/#visual-sanity-check","title":"Visual sanity check","text":"<p>You can visualize a batch of images and their labels:</p> <pre><code>import torch\nfrom fakeartdetector.data import cifake, show_image_and_target\n\ntrain_set, _ = cifake()\nimages, targets = next(iter(torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)))\nshow_image_and_target(images, targets)\n</code></pre>"},{"location":"reference/data/#api-reference","title":"API reference","text":""},{"location":"reference/data/#fakeartdetector.data.preprocess_data","title":"fakeartdetector.data.preprocess_data","text":"<pre><code>preprocess_data(processed_dir: str) -&gt; None\n</code></pre> <p>Downloads CIFAKE from Hugging Face, transforms to tensors, and saves to processed_dir for DVC tracking.</p> Source code in <code>src/fakeartdetector/data.py</code> <pre><code>def preprocess_data(processed_dir: str) -&gt; None:\n    \"\"\"\n    Downloads CIFAKE from Hugging Face, transforms to tensors,\n    and saves to processed_dir for DVC tracking.\n    \"\"\"\n    path = Path(processed_dir)\n    path.mkdir(parents=True, exist_ok=True)\n\n    typer.echo(\"Loading dataset from Hugging Face...\")\n    dataset = load_dataset(\"dragonintelligence/CIFAKE-image-dataset\")\n\n    # Define the transform: Resize (just in case), ToTensor (scales to 0-1)\n    transform = transforms.Compose(\n        [\n            transforms.Resize((32, 32)),\n            transforms.ToTensor(),\n        ]\n    )\n\n    for split in [\"train\", \"test\"]:\n        typer.echo(f\"Processing {split} split...\")\n        images = []\n        labels = []\n\n        for item in dataset[split]:\n            # item[\"image\"] is a PIL object\n            img = transform(item[\"image\"].convert(\"RGB\"))\n            images.append(img)\n            labels.append(item[\"label\"])\n\n        # Stack into [N, 3, 32, 32]\n        img_tensor = torch.stack(images)\n        label_tensor = torch.tensor(labels).long()\n\n        # Save tensors\n        torch.save(img_tensor, path / f\"{split}_images.pt\")\n        torch.save(label_tensor, path / f\"{split}_target.pt\")\n\n    typer.echo(f\"Finished! Data saved in {processed_dir}\")\n</code></pre>"},{"location":"reference/data/#fakeartdetector.data.cifake","title":"fakeartdetector.data.cifake","text":"<pre><code>cifake(processed_dir: str = 'data/processed') -&gt; tuple[torch.utils.data.TensorDataset, torch.utils.data.TensorDataset]\n</code></pre> <p>Return train and test dataloaders for CIFAKE.</p> Source code in <code>src/fakeartdetector/data.py</code> <pre><code>def cifake(\n    processed_dir: str = \"data/processed\",\n) -&gt; tuple[torch.utils.data.TensorDataset, torch.utils.data.TensorDataset]:\n    \"\"\"Return train and test dataloaders for CIFAKE.\"\"\"\n\n    path = Path(processed_dir)\n\n    required_files = {\n        \"train_images\": path / \"train_images.pt\",\n        \"train_target\": path / \"train_target.pt\",\n        \"test_images\": path / \"test_images.pt\",\n        \"test_target\": path / \"test_target.pt\",\n    }\n\n    # this is just a check\n    for name, path in required_files.items():\n        if not path.is_file():\n            raise FileNotFoundError(\n                f\"Preprocessed data file '{path}' not found. \"\n                \"Please run preprocess_data(processed_dir='data/processed') before calling cifake().\"\n            )\n\n    train_images = torch.load(required_files[\"train_images\"], weights_only=True)\n    train_target = torch.load(required_files[\"train_target\"], weights_only=True)\n    test_images = torch.load(required_files[\"test_images\"], weights_only=True)\n    test_target = torch.load(required_files[\"test_target\"], weights_only=True)\n\n    train_set = torch.utils.data.TensorDataset(train_images, train_target)\n    test_set = torch.utils.data.TensorDataset(test_images, test_target)\n\n    return train_set, test_set\n</code></pre>"},{"location":"reference/data/#fakeartdetector.data.normalize","title":"fakeartdetector.data.normalize","text":"<pre><code>normalize(images: torch.Tensor) -&gt; torch.Tensor\n</code></pre> <p>Standard normalization for image tensors.</p> Source code in <code>src/fakeartdetector/data.py</code> <pre><code>def normalize(images: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Standard normalization for image tensors.\"\"\"\n    # Using per-channel mean/std is standard for RGB\n    return (images - images.mean(dim=(0, 2, 3), keepdim=True)) / images.std(dim=(0, 2, 3), keepdim=True)\n</code></pre>"},{"location":"reference/data/#fakeartdetector.data.show_image_and_target","title":"fakeartdetector.data.show_image_and_target","text":"<pre><code>show_image_and_target(images: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Plot RGB images and their labels in a grid.</p> Source code in <code>src/fakeartdetector/data.py</code> <pre><code>def show_image_and_target(images: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"Plot RGB images and their labels in a grid.\"\"\"\n    row_col = int(len(images) ** 0.5)\n    fig = plt.figure(figsize=(10.0, 10.0))\n    grid = ImageGrid(fig, 111, nrows_ncols=(row_col, row_col), axes_pad=0.3)\n\n    for ax, im, label in zip(grid, images, target):\n        # Permute from [C, H, W] to [H, W, C] for matplotlib\n        ax.imshow(im.permute(1, 2, 0))\n        ax.set_title(f\"Label: {'Fake' if label.item() == 1 else 'Real'}\")\n        ax.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"reference/evaluate/","title":"<code>Evaluate</code>","text":"<p>This module evaluates a saved model checkpoint on the CIFAKE test split.</p>"},{"location":"reference/evaluate/#what-this-module-does","title":"What this module does","text":"<ul> <li>Loads a <code>FakeArtClassifier</code> and applies a saved <code>state_dict</code></li> <li>Loads the processed CIFAKE tensors via <code>cifake()</code></li> <li>Runs inference on the test set and prints accuracy</li> <li>Selects the best available device in this order: CUDA \u2192 MPS \u2192 CPU</li> </ul>"},{"location":"reference/evaluate/#evaluate-a-checkpoint","title":"Evaluate a checkpoint","text":"<p>The evaluation entrypoint expects a path to a PyTorch <code>state_dict</code> checkpoint (as saved by <code>torch.save(model.state_dict(), ...)</code>).</p> <p>Run via the Invoke task (recommended):</p> <pre><code>uv run invoke evaluate\n</code></pre> <p>Or run the module directly:</p> <pre><code>uv run python -m fakeartdetector.evaluate --help\n</code></pre> <p>Each evaluation run writes a Loguru log file into Hydra's per-run output folder:</p> <ul> <li><code>outputs/.../evaluate.log</code></li> </ul> <p>By default, evaluation uses <code>cfg.evaluate.model_checkpoint</code> (which defaults to <code>cfg.dataset.savedTo.path</code>). You can override it:</p> <pre><code>uv run python -m fakeartdetector.evaluate --model-checkpoint models/base_model.pth\n</code></pre> <p>If you want to evaluate the best Lightning checkpoint (saved in the Hydra output directory), pass the path recorded in <code>artifacts.yaml</code>:</p> <pre><code># Example using a run's output directory\nBEST=$(yq '.best_checkpoint_path' outputs/2026-01-10/14-31-25/artifacts.yaml)\nuv run python -m fakeartdetector.evaluate --model-checkpoint \"$BEST\"\n</code></pre>"},{"location":"reference/evaluate/#how-predictions-are-computed","title":"How predictions are computed","text":"<p>The model outputs logits with shape $(B, 1)$. Evaluation converts logits to class predictions using a sigmoid threshold at 0.5:</p> <pre><code>y_pred = (sigmoid(model(img).squeeze(1)) &gt; 0.5).long()\n</code></pre> <p>Accuracy is computed as the fraction of correct predictions over the full test set.</p>"},{"location":"reference/evaluate/#api-reference","title":"API reference","text":""},{"location":"reference/evaluate/#fakeartdetector.evaluate.evaluate_checkpoint","title":"fakeartdetector.evaluate.evaluate_checkpoint","text":"<pre><code>evaluate_checkpoint(model_checkpoint: str, batch_size: int = 32, threshold: float = 0.5) -&gt; float\n</code></pre> <p>Evaluate a trained model checkpoint and return accuracy.</p> Source code in <code>src/fakeartdetector/evaluate.py</code> <pre><code>def evaluate_checkpoint(model_checkpoint: str, batch_size: int = 32, threshold: float = 0.5) -&gt; float:\n    \"\"\"Evaluate a trained model checkpoint and return accuracy.\"\"\"\n    logger.info(\"Evaluating model\")\n    logger.info(f\"checkpoint: {model_checkpoint}\")\n    logger.info(f\"batch_size: {batch_size}, threshold: {threshold}\")\n\n    model = FakeArtClassifier().to(DEVICE)\n    state_dict = load(model_checkpoint, map_location=DEVICE)\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    _, test_set = cifake()\n    test_dataloader = DataLoader(test_set, batch_size=batch_size)\n\n    correct, total = 0, 0\n    with no_grad():\n        for img, target in test_dataloader:\n            img, target = img.to(DEVICE), target.to(DEVICE)\n            y_pred = (sigmoid(model(img).squeeze(1)) &gt; threshold).long()\n            correct += (y_pred == target).sum().item()\n            total += target.size(0)\n    accuracy = correct / total\n    logger.info(f\"Test accuracy: {accuracy}\")\n    return accuracy\n</code></pre>"},{"location":"reference/evidently/","title":"Evidently Reports","text":"<p>Evidently is used to generate data and model monitoring reports. The project stores generated reports in <code>reports/evidently_reports/</code>.</p>"},{"location":"reference/evidently/#generating-reports","title":"Generating reports","text":"<ul> <li>See <code>src/fakeartdetector/evidently_report.py</code> for how reports are created.</li> <li>Typical flow:</li> <li>Collect recent inference logs or dataset slices</li> <li>Run the report script to produce interactive HTML reports</li> </ul>"},{"location":"reference/evidently/#viewing-reports","title":"Viewing reports","text":"<ul> <li>HTML reports are stored in <code>reports/evidently_reports/</code> and can be opened in a browser.</li> </ul>"},{"location":"reference/evidently/#use-cases","title":"Use cases","text":"<ul> <li>Model drift detection</li> <li>Data quality checks</li> <li>Performance comparisons between models</li> </ul>"},{"location":"reference/frontend/","title":"<code>Frontend (Streamlit)</code>","text":"<p>This project includes a Streamlit-based frontend used to upload images and request classification from the backend API.</p>"},{"location":"reference/frontend/#purpose","title":"Purpose","text":"<ul> <li>Provide an easy UI for model selection and inference.</li> <li>Allow downloading of inference logs and database.</li> </ul>"},{"location":"reference/frontend/#running-locally","title":"Running locally","text":"<p>Install project dependencies with the repository's package manager, then run:</p> <pre><code>uv sync\nstreamlit run src/fakeartdetector/frontend.py\n</code></pre>"},{"location":"reference/frontend/#environment-variables","title":"Environment variables","text":"<ul> <li><code>USE_LOCAL</code>: if set, the frontend will use <code>http://127.0.0.1:8000</code> as the backend.</li> <li><code>BACKEND</code> or <code>BACKEND_URL</code>: override the backend URL (e.g., <code>https://example.com</code>).</li> </ul>"},{"location":"reference/frontend/#behavior","title":"Behavior","text":"<ul> <li>Fetches model list from the backend <code>/wandb-models</code> endpoint and shows latest models.</li> <li>Allows switching backend model via <code>/switch-model</code> when a model is selected.</li> <li>Uploads images to <code>/model/</code> for prediction.</li> <li>Lists and downloads inference logs from <code>/inference-logs/files</code> and <code>/download-db</code>.</li> </ul>"},{"location":"reference/frontend/#deployment-notes","title":"Deployment notes","text":"<ul> <li>The frontend attempts to discover a Cloud Run service when not running locally. Configure <code>BACKEND</code> in production for reliability.</li> <li>Use Streamlit's <code>st.cache_resource</code> (already used) to cache long-lived objects like backend discovery and thread pools.</li> </ul> <p>See the source: <code>src/fakeartdetector/frontend.py</code> for implementation details.</p>"},{"location":"reference/model/","title":"<code>Model</code>","text":"<p>This module defines the CNN used for classifying images from the CIFAKE dataset.</p>"},{"location":"reference/model/#what-this-module-does","title":"What this module does","text":"<ul> <li>Provides <code>FakeArtClassifier</code>, a PyTorch Lightning <code>LightningModule</code></li> <li>Implements a small CNN backbone + MLP classifier + 1-unit logit head</li> <li>Exposes a binary-classification forward pass that returns raw logits</li> </ul>"},{"location":"reference/model/#model-io","title":"Model I/O","text":"<ul> <li>Input: RGB images with shape $(B, 3, 32, 32)$</li> <li>Output: logits with shape $(B, 1)$</li> </ul> <p>The model is trained/evaluated as a binary classifier by applying a sigmoid and thresholding:</p> <pre><code>from torch import sigmoid\n\nlogits = model(images).squeeze(1)          # (B,)\npreds = (sigmoid(logits) &gt; 0.5).long()     # (B,)\n</code></pre>"},{"location":"reference/model/#loss-and-training-notes","title":"Loss and training notes","text":"<p>The module defines <code>self.criterium = torch.nn.BCEWithLogitsLoss()</code>.</p> <p>If you use <code>BCEWithLogitsLoss</code>, targets must be floats (0.0/1.0) and match the logits shape. In this repository, the training script squeezes logits and casts targets:</p> <pre><code>logits = model(images).squeeze(1)\nloss = model.criterium(logits, targets.float())\n</code></pre>"},{"location":"reference/model/#model-reference","title":"Model reference","text":""},{"location":"reference/model/#fakeartdetector.model.FakeArtClassifier","title":"fakeartdetector.model.FakeArtClassifier","text":"<p>               Bases: <code>LightningModule</code></p> <p>CNN for classifying AI-generated vs human-created art (CIFAKE dataset)</p>"},{"location":"reference/model/#fakeartdetector.model.FakeArtClassifier--assumptions","title":"Assumptions","text":"<pre><code>- Input images are RGB with shape (3, 32, 32).\n- The task is binary classification with two mutually exclusive classes.\n- The model outputs raw logits suitable for `CrossEntropyLoss`.\n</code></pre> Architecture <ul> <li> <p>Input: RGB image (3 \u00d7 32 \u00d7 32)</p> </li> <li> <p>Backbone (Feature Extractor):</p> <ul> <li>Block 1: Conv2d(3 \u2192 32) + BatchNorm + LeakyReLU + MaxPool (32 \u2192 16)</li> <li>Block 2: Conv2d(32 \u2192 64) + BatchNorm + LeakyReLU + MaxPool (16 \u2192 8)</li> <li>Block 3: Conv2d(64 \u2192 128) + BatchNorm + LeakyReLU + MaxPool (8 \u2192 4)</li> </ul> </li> <li> <p>Classifier:</p> <ul> <li>Flatten (128 \u00d7 4 \u00d7 4 = 2048)</li> <li>Linear(2048 \u2192 256) + LeakyReLU + Dropout(0.3)</li> <li>Linear(256 \u2192 128) + LeakyReLU</li> </ul> </li> <li> <p>Head:</p> <ul> <li>Linear(128 \u2192 1)</li> </ul> </li> <li> <p>Output:</p> <ul> <li>Single logit for binary classification (e.g., Real vs Fake)</li> </ul> </li> </ul>"},{"location":"reference/model/#fakeartdetector.model.FakeArtClassifier--loss-function","title":"Loss Function","text":"<pre><code>- Trained using BCEWithLogitsLoss\n</code></pre>"},{"location":"reference/model/#fakeartdetector.model.FakeArtClassifier--notes","title":"Notes","text":"<pre><code>- Dropout is used to reduce overfitting.\n- The architecture is lightweight and suitable for small image datasets.\n</code></pre>"},{"location":"reference/model/#fakeartdetector.model.FakeArtClassifier--inference","title":"Inference","text":"<pre><code>- Please pass the output with (torch.sigmoid(model(data)) &gt;0.5).long()\n</code></pre> Source code in <code>src/fakeartdetector/model.py</code> <pre><code>class FakeArtClassifier(LightningModule):\n    \"\"\"CNN for classifying AI-generated vs human-created art (CIFAKE dataset)\n\n    Assumptions\n    -----------\n        - Input images are RGB with shape (3, 32, 32).\n        - The task is binary classification with two mutually exclusive classes.\n        - The model outputs raw logits suitable for `CrossEntropyLoss`.\n\n    Architecture:\n        - Input: RGB image (3 \u00d7 32 \u00d7 32)\n\n        - Backbone (Feature Extractor):\n            - Block 1: Conv2d(3 \u2192 32) + BatchNorm + LeakyReLU + MaxPool (32 \u2192 16)\n            - Block 2: Conv2d(32 \u2192 64) + BatchNorm + LeakyReLU + MaxPool (16 \u2192 8)\n            - Block 3: Conv2d(64 \u2192 128) + BatchNorm + LeakyReLU + MaxPool (8 \u2192 4)\n\n        - Classifier:\n            - Flatten (128 \u00d7 4 \u00d7 4 = 2048)\n            - Linear(2048 \u2192 256) + LeakyReLU + Dropout(0.3)\n            - Linear(256 \u2192 128) + LeakyReLU\n\n        - Head:\n            - Linear(128 \u2192 1)\n\n\n        - Output:\n            - Single logit for binary classification (e.g., Real vs Fake)\n\n    Loss Function\n    -------------\n        - Trained using BCEWithLogitsLoss\n\n    Notes\n    -----\n        - Dropout is used to reduce overfitting.\n        - The architecture is lightweight and suitable for small image datasets.\n    Inference\n    ---------\n        - Please pass the output with (torch.sigmoid(model(data)) &gt;0.5).long()\n    \"\"\"\n\n    def __init__(self, lr: float = 0.005, dropout: float = 0.1, optimizer_cfg: Any | None = None) -&gt; None:\n        super().__init__()\n\n        self.dropout = dropout\n        self.lr = lr\n        self.optimizer_cfg = optimizer_cfg\n\n        self.save_hyperparameters({\"lr\": lr, \"dropout\": dropout})\n\n        self.backbone = nn.Sequential(\n            # Block 1: 3 -&gt; 32 channels\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size=2),  # 32x32 -&gt; 16x16\n            # Block 2: 32 -&gt; 64 channels\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size=2),  # 16x16 -&gt; 8x8\n            # Block 3: 64 -&gt; 128 channels\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size=2),  # 8x8 -&gt; 4x4\n        )\n\n        # Classification layers\n        # After 3 pooling layers: 128 channels * 4 * 4 = 2048\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(2048, 256),\n            nn.LeakyReLU(),\n            nn.Dropout(p=self.dropout),\n            nn.Linear(256, 128),\n            nn.LeakyReLU(),\n        )\n        self.head = nn.Linear(128, 1)  # 2 classes: Real vs Fake\n\n        self.criterium = nn.BCEWithLogitsLoss()\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        if x.ndim != 4:\n            raise ValueError(\"Expected input to be a 4D tensor [Batch, C, H, W]\")\n        if x.shape[1] != 3:\n            raise ValueError(\"Expected 3 input channels (RGB)\")\n        return self.head(self.classifier(self.backbone(x)))\n\n    def training_step(self, batch, batch_idx):\n        data, target = batch\n        logits = self(data).squeeze(1)\n        target = target.to(dtype=logits.dtype).view_as(logits)\n\n        loss = self.criterium(logits, target)\n\n        preds = (logits &gt; 0).to(dtype=target.dtype)\n        acc = (preds == target).float().mean()\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        data, target = batch\n        logits = self(data).squeeze(1)\n        target = target.to(dtype=logits.dtype).view_as(logits)\n\n        loss = self.criterium(logits, target)\n        preds = (logits &gt; 0).to(dtype=target.dtype)\n        acc = (preds == target).float().mean()\n\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        if self.optimizer_cfg is not None:\n            # Local import so unit tests importing the model don't require Hydra.\n            from hydra.utils import instantiate\n\n            return instantiate(self.optimizer_cfg, params=self.parameters())\n        return optim.Adam(self.parameters(), lr=self.lr)\n</code></pre>"},{"location":"reference/onnx/","title":"<code>ONNX Export</code>","text":"<p>This project includes tools to export the trained PyTorch Lightning model to ONNX format for inference with ONNX Runtime.</p>"},{"location":"reference/onnx/#purpose","title":"Purpose","text":"<ul> <li>Create a portable representation of the model for low-latency or cross-platform inference.</li> </ul>"},{"location":"reference/onnx/#how-to-export","title":"How to export","text":"<ul> <li>See <code>src/fakeartdetector/export_onnx.py</code> for the export script.</li> <li>Typical usage (example):</li> </ul> <pre><code>python src/fakeartdetector/export_onnx.py --checkpoint ./staged_model_dir/model.ckpt --output model.onnx\n</code></pre>"},{"location":"reference/onnx/#notes-compatibility","title":"Notes &amp; compatibility","text":"<ul> <li>Ensure the model is in evaluation mode prior to export.</li> <li>ONNX ops supported depend on the PyTorch and ONNX versions; test the exported model with ONNX Runtime.</li> </ul>"},{"location":"reference/onnx/#running-onnx-model","title":"Running ONNX model","text":"<p>Install dependencies via the project's package manager and run ONNX Runtime checks:</p> <pre><code>uv sync\nuv run python -c \"import onnxruntime as ort; print(ort.get_device())\"\n</code></pre> <p>Then write a small runner that loads <code>model.onnx</code> and runs inference on preprocessed tensors (run it via <code>uv run</code>).</p>"},{"location":"reference/tasks_invocations/","title":"Tasks &amp; Invocations","text":"<p>This document explains how to use the project's Invoke tasks (the <code>tasks.py</code> file in the repository root). It lists every task, explains common options, and provides many examples and troubleshooting tips.</p> <p>Table of contents</p> <ul> <li>Quickstart</li> <li>How Invoke is used in this repo</li> <li>Common patterns and environment notes</li> <li>Detailed task reference (examples and flags)</li> <li>Troubleshooting</li> <li>CI / Automation tips</li> </ul>"},{"location":"reference/tasks_invocations/#quickstart","title":"Quickstart","text":"<ol> <li>Install project dependencies using the repository package manager:</li> </ol> <pre><code>uv sync          # install runtime dependencies\nuv sync --dev    # (optional) install developer dependencies\n</code></pre> <ol> <li>List available tasks:</li> </ol> <pre><code>inv --list\n</code></pre> <ol> <li>Run a task, for example training:</li> </ol> <pre><code>inv train --epochs 2 --lr 0.001\n</code></pre> <p>The <code>inv</code> CLI invokes the tasks defined in <code>tasks.py</code>.</p>"},{"location":"reference/tasks_invocations/#how-invoke-is-used-in-this-repo","title":"How Invoke is used in this repo","text":"<ul> <li><code>tasks.py</code> contains developer convenience tasks to run common workflows (training, evaluation,   docs, docker builds, etc.).</li> <li>The tasks generally forward options to the project's Typer/Hydra CLIs (e.g.,   <code>python -m fakeartdetector.train</code>) or call external tools (e.g., <code>docker</code>, <code>dvc</code>, <code>streamlit</code>).</li> <li>Many tasks use the <code>uv run</code> prefix. <code>uv</code> is the project's packaging/runtime helper; if <code>uv</code> is not   present in your environment, substitute the underlying command (e.g., run   <code>python -m fakeartdetector.train</code> directly).</li> </ul>"},{"location":"reference/tasks_invocations/#common-patterns-and-environment-notes","title":"Common patterns and environment notes","text":"<ul> <li>On macOS, <code>num_workers</code> for data loaders is often set to <code>0</code> to avoid multiprocessing issues.</li> <li>Precision flags: the repo supports <code>32</code>, <code>bf16-mixed</code>, <code>bf16</code>, and <code>bf16-true</code> for training. Use   <code>inv list_precisions</code> to see short descriptions.</li> <li>For local development of the API, use <code>inv startapi</code> and access <code>http://127.0.0.1:8000</code>.</li> <li>To run the Streamlit frontend locally, use <code>inv frontend</code> (optionally <code>--browser True</code>).</li> </ul>"},{"location":"reference/tasks_invocations/#detailed-task-reference","title":"Detailed task reference","text":"<p>All tasks live in <code>tasks.py</code>. Below are the commonly used tasks with their flags and examples.</p> <ul> <li><code>preprocess-data</code></li> <li>Purpose: Run the data preprocessing pipeline.</li> <li>Flags:<ul> <li><code>--out-dir</code> (default: <code>data/processed</code>) \u2014 output directory for processed data.</li> </ul> </li> <li>Example:</li> </ul> <pre><code>inv preprocess-data --out-dir data/processed\n</code></pre> <ul> <li><code>startapi</code></li> <li>Purpose: Start the FastAPI server via <code>uvicorn</code> for local development.</li> <li>Flags:<ul> <li><code>--host</code> (default: <code>127.0.0.1</code>)</li> <li><code>--port</code> (default: <code>8000</code>)</li> </ul> </li> <li>Example:</li> </ul> <pre><code>inv startapi --host 0.0.0.0 --port 8000\n</code></pre> <ul> <li><code>train</code></li> <li>Purpose: Launch the training CLI (<code>fakeartdetector.train</code>) which is Hydra/Typer-backed.</li> <li>Flags (selected): <code>--lr</code>, <code>--epochs</code>, <code>--batch-size</code>, <code>--num-workers</code>, <code>--precision</code>,     <code>--profiler</code>, <code>--experiment</code>, <code>--dataset</code>, <code>--logging</code>, <code>--optimizer</code>, <code>--config-name</code>,     <code>--print-config</code>.</li> <li>Precision options: <code>32</code>, <code>bf16-mixed</code>, <code>bf16</code>, <code>bf16-true</code>. Use <code>inv list_precisions</code> to see     descriptions.</li> <li>Examples:</li> </ul> <pre><code>inv train --epochs 10 --lr 0.001 --batch-size 64\ninv train --experiment trials1 --precision bf16-mixed\n</code></pre> <ul> <li><code>train-help</code></li> <li>Purpose: Print the CLI help for the training module (Typer/Hydra flags).</li> <li>Example:</li> </ul> <pre><code>inv train-help\n</code></pre> <ul> <li><code>list-configs</code></li> <li>Purpose: List available config group options under <code>configs/</code> (experiment, dataset, logging,     optimizer, profiler).</li> <li>Example:</li> </ul> <pre><code>inv list-configs\n</code></pre> <ul> <li><code>list-precisions</code></li> <li>Purpose: Print supported precision choices for training.</li> <li>Example:</li> </ul> <pre><code>inv list-precisions\n</code></pre> <ul> <li><code>tensorboard</code></li> <li>Purpose: Start TensorBoard pointing to the project's <code>outputs</code> directory or another logs     directory.</li> <li>Flags: <code>--logdir</code>, <code>--port</code>.</li> <li>Example:</li> </ul> <pre><code>inv tensorboard --logdir outputs/2026-01-14/17-45-00 --port 6006\n</code></pre> <ul> <li><code>test</code></li> <li>Purpose: Run <code>pytest</code> with coverage.</li> <li>Flags: <code>--pattern</code> (run a matching file or folder pattern; default <code>tests/</code>).</li> <li>Examples:</li> </ul> <pre><code>inv test\ninv test --pattern tests/test_api.py\n</code></pre> <ul> <li><code>frontend</code></li> <li>Purpose: Start the Streamlit frontend.</li> <li>Flags: <code>--browser</code> (open browser after starting).</li> <li>Example:</li> </ul> <pre><code>inv frontend --browser True\n</code></pre> <ul> <li><code>evaluate</code></li> <li>Purpose: Run evaluation CLI (<code>fakeartdetector.evaluate</code>).</li> <li>Flags (selected): <code>--checkpoint</code>, <code>--batch-size</code>, <code>--threshold</code>, <code>--config-name</code>, <code>--dataset</code>,     <code>--evaluate-cfg</code>, <code>--print-config</code>.</li> <li>Example:</li> </ul> <pre><code>inv evaluate --checkpoint outputs/latest/model.ckpt --batch-size 64\n</code></pre> <ul> <li><code>visualize</code></li> <li>Purpose: Run embedding visualization CLI (<code>fakeartdetector.visualize</code>).</li> <li>Flags: <code>--checkpoint</code>, <code>--figure-name</code>, <code>--output-dir</code>, <code>--data-dir</code>, <code>--batch-size</code>, <code>--pca-*</code>,     <code>--tsne-*</code>, <code>--seed</code>.</li> <li>Example:</li> </ul> <pre><code>inv visualize --checkpoint models/base_model.pth --figure-name embeddings.png\n</code></pre> <ul> <li><code>docker-build</code> / <code>docker-build-api</code></li> <li>Purpose: Build Docker images for training and the API.</li> <li>Flags: <code>--progress</code> (docker build progress mode: <code>plain|auto|tty</code>).</li> <li>Example:</li> </ul> <pre><code>inv docker-build --progress plain\ninv docker-build-api --progress plain\n</code></pre> <ul> <li><code>dvc</code> / <code>pull-data</code></li> <li><code>dvc</code>: Add a folder to DVC, commit, and push remote storage. Flags: <code>--folder</code>, <code>--message</code>.</li> <li><code>pull-data</code>: Run <code>dvc pull</code> to fetch artifacts.</li> <li>Examples:</li> </ul> <pre><code>inv dvc --folder data/raw --message \"add raw images\"\ninv pull-data\n</code></pre> <ul> <li><code>build-docs</code> / <code>serve-docs</code></li> <li>Purpose: Build or serve MkDocs documentation using <code>docs/mkdocs.yaml</code>.</li> <li>Examples:</li> </ul> <pre><code>inv build-docs\ninv serve-docs\n</code></pre> <ul> <li><code>make-req-txt</code></li> <li>Purpose: Export runtime and dev dependencies to <code>requirements.txt</code> and <code>requirements_dev.txt</code>.</li> <li>Example:</li> </ul> <pre><code>inv make-req-txt\n</code></pre> <ul> <li><code>push</code></li> <li>Purpose: Add all changes, commit with a message, and push to the current branch (sets upstream     if needed).</li> <li>Flags: <code>--message</code>.</li> <li>Example:</li> </ul> <pre><code>inv push --message \"fix: update README\"\n</code></pre>"},{"location":"reference/tasks_invocations/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If a task fails with <code>uv: command not found</code>, you can run the underlying command directly. For   example, replace <code>uv run python -m fakeartdetector.train</code> with <code>python -m fakeartdetector.train</code>.</li> <li>On macOS, if multiprocessing data loaders crash, set <code>--num-workers 0</code> when training.</li> <li>If Docker builds fail on macOS with permissions, ensure you have the correct Docker context and   resources.</li> <li>If Streamlit doesn't open the browser, start it with <code>inv frontend --browser True</code> or open the   displayed URL manually.</li> </ul>"},{"location":"reference/tasks_invocations/#ci-automation-tips","title":"CI / Automation tips","text":"<ul> <li>Use <code>inv test --pattern tests/test_api.py</code> to run only a focused API test in CI for quick   feedback.</li> <li>Use <code>inv make-req-txt</code> in a job that regenerates pinned requirements after dependency changes.</li> <li>Use <code>inv build-docs</code> as a step to validate documentation builds before deployment.</li> </ul>"},{"location":"reference/tasks_invocations/#automatic-task-api-reference","title":"Automatic task API reference","text":"<p>The sections below use mkdocstrings to embed the <code>tasks.py</code> function docstrings directly. This keeps the reference in-sync with the source and surfaces the function signature and docstring for each Invoke task.</p>"},{"location":"reference/tests/","title":"<code>Tests</code>","text":"<p>This page describes the test suite and how to run tests.</p>"},{"location":"reference/tests/#running-tests","title":"Running tests","text":"<p>From the project root run (use the repository package manager). To install runtime and developer dependencies and run tests:</p> <pre><code>uv sync\nuv sync --dev   # (optional) install development dependencies\nuv run pytest -q\n# or just invoke\nuv run invoke test #(more on the invocations documentation page)\n</code></pre>"},{"location":"reference/tests/#test-structure","title":"Test structure","text":"<ul> <li>Tests live in the <code>tests/</code> directory.</li> <li><code>tests/test_api.py</code> uses <code>fastapi.testclient.TestClient</code> to exercise the API endpoints and patches environment variables.</li> </ul>"},{"location":"reference/tests/#writing-tests","title":"Writing tests","text":"<ul> <li>Use <code>pytest</code> and fixtures for reusable setup.</li> <li>Use <code>unittest.mock.patch</code> to stub external dependencies (e.g., GCS, wandb, or the model callable).</li> <li>When testing FastAPI endpoints, use <code>TestClient(app)</code> and assert status codes and JSON bodies.</li> </ul>"},{"location":"reference/tests/#notes","title":"Notes","text":"<ul> <li>Tests may interact with SQLite files; ensure test fixtures clean or isolate test DB paths.</li> <li>Keep network calls mocked to avoid flakiness.</li> </ul>"},{"location":"reference/train/","title":"<code>Training</code>","text":"<p>Training is driven by Hydra configs and executed with PyTorch Lightning's <code>pl.Trainer</code>, exposed via an Invoke task + Typer CLI.</p>"},{"location":"reference/train/#what-training-does","title":"What training does","text":"<ul> <li>Loads the processed CIFAKE tensors via <code>cifake()</code></li> <li>Trains <code>FakeArtClassifier</code> using Lightning <code>pl.Trainer</code></li> <li>Optimizer is configured via Hydra (default: Adam), loss: <code>BCEWithLogitsLoss</code></li> <li>Supports precision control (e.g., <code>32</code>, <code>bf16-mixed</code>) and data loader <code>num_workers</code></li> <li>Saves the trained model <code>state_dict</code> to <code>cfg.dataset.savedTo.path</code></li> <li>Writes per-run artifacts in Hydra's output directory (config + metrics + checkpoints)</li> </ul>"},{"location":"reference/train/#train-the-model","title":"Train the model","text":"<p>Run via the Invoke task (recommended):</p> <pre><code>uv run invoke train\n</code></pre> <p>Show available training options:</p> <pre><code>uv run invoke train --help\n</code></pre> <p>Examples:</p> <pre><code># Switch optimizer from config\nuv run invoke train --optimizer sgd\n\n# Override precision and number of data loader workers\nuv run invoke train --precision 32 --num-workers 0\nuv run invoke train --precision bf16-mixed --epochs 5\n\n# List available precision options (macOS/MPS compatible)\nuv run invoke list_precisions\n\n# List available config group choices\nuv run invoke list-configs\n</code></pre> <p>Or run the module directly:</p> <pre><code>uv run python -m fakeartdetector.train --help\n</code></pre>"},{"location":"reference/train/#outputs","title":"Outputs","text":"<p>After training, outputs include:</p> <ul> <li>Model state dict at <code>cfg.dataset.savedTo.path</code> (e.g., <code>models/base_model.pth</code>)</li> <li>Hydra output directory: <code>outputs/&lt;date&gt;/&lt;time&gt;/</code></li> <li><code>config_full.yaml</code>: full composed Hydra config for reproducibility</li> <li><code>artifacts.yaml</code>: paths to saved model, best checkpoint, logs, and settings (e.g., <code>num_workers</code>)</li> <li><code>lightning/version_0/metrics.csv</code>: CSV logs with <code>train_loss</code>, <code>train_acc</code>, <code>val_loss</code>, <code>val_acc</code>, etc.</li> <li><code>tensorboard/</code>: TensorBoard event files for visualization</li> <li><code>checkpoints/</code>: Lightning checkpoints (best model by <code>val_loss</code>)</li> <li><code>train_hydra.log</code>: Loguru training log</li> <li><code>profiler/</code>: Profiler output (if profiling enabled)</li> </ul>"},{"location":"reference/train/#training-details","title":"Training details","text":"<ul> <li>Device selection: CUDA \u2192 MPS \u2192 CPU</li> <li>Optimizer: configurable via <code>cfg.optimizer</code> (default: Adam)</li> <li>Loss: <code>BCEWithLogitsLoss</code></li> <li>Precision: configurable via <code>experiment.hyperparameters.precision</code> (default: <code>32</code>). Common values: <code>32</code>, <code>bf16-mixed</code> (recommended on MPS/macOS). Note: <code>16-mixed</code> is not supported on Apple MPS.</li> <li>Data loading: set <code>experiment.hyperparameters.num_workers</code> or <code>--num-workers</code> to control parallelism. On macOS MPS, <code>0</code> workers is often fastest.</li> <li>Predictions for accuracy: <code>preds = (logits &gt; 0).long()</code> (equivalent to sigmoid threshold at 0.5)</li> </ul> <p>Note: Use <code>uv run invoke train --print-config</code> to print the resolved Hydra config used for the run.</p>"},{"location":"reference/train/#profiling","title":"Profiling","text":"<p>Profile training performance using built-in profilers:</p> <pre><code># Advanced profiler (recommended) - shows timing per Lightning hook\nuv run invoke train --profiler advanced --epochs 1\n\n# PyTorch profiler - generates Chrome trace (may hang on macOS)\nuv run invoke train --profiler pytorch --epochs 1\n\n# Simple profiler - lightweight wall-time tracking\nuv run invoke train --profiler simple --epochs 1\n\n# List available profiler options\nuv run invoke list-configs\n</code></pre> <p>Advanced Profiler Output:</p> <ul> <li>Prints timing summary to terminal after training</li> <li>Saves to <code>outputs/&lt;date&gt;/&lt;time&gt;/profiler/advanced_profiler.txt</code></li> <li>Shows time spent in <code>training_step</code>, <code>validation_step</code>, <code>optimizer_step</code>, etc.</li> </ul> <p>PyTorch Profiler Output:</p> <ul> <li>Saves trace to <code>outputs/&lt;date&gt;/&lt;time&gt;/profiler/trace.json</code></li> <li>View in Chrome: open <code>chrome://tracing</code> and load the trace file</li> <li>Note: May hang on macOS MPS; use advanced profiler instead</li> </ul>"},{"location":"reference/train/#viewing-training-metrics-with-tensorboard","title":"Viewing Training Metrics with TensorBoard","text":"<p>Training automatically logs to TensorBoard. View real-time or historical metrics:</p> <pre><code># View all training runs\nuv run invoke tensorboard\n\n# View specific run\nuv run invoke tensorboard --logdir outputs/2026-01-10/14-31-25/tensorboard\n\n# Custom port\nuv run invoke tensorboard --port 6007\n</code></pre> <p>Then open <code>http://localhost:6006</code> in your browser.</p> <p>TensorBoard Views:</p> <ul> <li>SCALARS: Training/validation loss and accuracy curves</li> <li>HPARAMS: Compare hyperparameters across runs</li> <li>PROFILE: GPU/CPU timeline (if using PyTorch profiler)</li> </ul>"},{"location":"reference/train/#using-saved-statistics","title":"Using saved statistics","text":"<p>You can print a summary from a finished run:</p> <pre><code>uv run invoke training-summary --output-dir outputs/2026-01-10/14-31-25\n</code></pre> <p>Or load metrics in Python:</p> <pre><code>from fakeartdetector.training_stats import load_metrics\n\ndf = load_metrics(\"outputs/2026-01-10/14-31-25\")\nprint(df[[\"train_loss\",\"val_loss\",\"train_acc\",\"val_acc\"]].dropna().tail())\n</code></pre>"},{"location":"reference/train/#api-reference","title":"API reference","text":""},{"location":"reference/train/#fakeartdetector.train.train_impl","title":"fakeartdetector.train.train_impl","text":"<pre><code>train_impl(cfg: DictConfig, print_config: bool = False) -&gt; None\n</code></pre> Source code in <code>src/fakeartdetector/train.py</code> <pre><code>def train_impl(cfg: DictConfig, print_config: bool = False) -&gt; None:\n    # set up the logging from the hydra output dir\n    output_dir = get_hydra_output_dir()\n    log_path = configure_loguru_file(output_dir, filename=\"train_hydra.log\", rotation=\"150 MB\")\n\n    # we can print the config if we want to\n    if print_config:\n        typer.echo(OmegaConf.to_yaml(cfg))\n\n    # here we put the config into the logs\n    logger.info(f\"Hydra output_dir: {output_dir}\")\n    logger.info(f\"Training log_path: {log_path}\")\n    logger.info(f\"Loading model with configuration: \\n {OmegaConf.to_yaml(cfg)}\")\n\n    hparams = cfg.experiment.hyperparameters\n    pl.seed_everything(int(hparams[\"seed\"]), workers=True)\n\n    logger.info(\"Training day and night\")\n    logger.info(\n        f\"lr: {hparams['lr']}, \"\n        f\"epochs: {hparams['epochs']}, \"\n        f\"batch_size: {hparams['batch_size']}, \"\n        f\"precision: {hparams.get('precision', '32')}\"\n    )\n\n    # Set up the LightningModule (Lightning will move it to the right device).\n    model = FakeArtClassifier(lr=hparams[\"lr\"], dropout=hparams[\"dropout\"], optimizer_cfg=cfg.optimizer)\n    logger.info(f\"Loaded Model onto memory: \\n{model}\")\n\n    # Handle Data Loading (GCS vs Local)\n    data_path = cfg.dataset.dataset.path\n    if data_path.startswith(\"gs://\"):\n        local_data_path = \"data/processed\"\n        download_folder_from_gcs(data_path, local_data_path)\n        train_set, val_set = cifake(processed_dir=local_data_path)\n    else:\n        train_set, val_set = cifake(processed_dir=data_path)\n\n    logger.info(\"Loaded training set\")\n\n    num_workers_cfg = hparams.get(\"num_workers\", None)\n    if num_workers_cfg is None:\n        try:\n            # This might be needed in the Cloud Run\n            available_cores = len(os.sched_getaffinity(0))\n        except AttributeError:\n            # Fallback for local\n            available_cores = os.cpu_count() or 1\n\n        # Cap at 4 since the process overhead is high for this small dataset\n        num_workers = min(4, max(0, available_cores - 1))\n    else:\n        num_workers = int(num_workers_cfg)\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=True,\n        num_workers=num_workers,\n        persistent_workers=num_workers &gt; 0,\n    )\n    val_loader = DataLoader(\n        val_set,\n        batch_size=hparams[\"batch_size\"],\n        shuffle=False,\n        num_workers=num_workers,\n        persistent_workers=num_workers &gt; 0,\n    )\n    logger.debug(\"Loaded train/val dataloaders\")\n\n    # Set up loggers\n    csv_logger = CSVLogger(save_dir=str(output_dir), name=\"lightning\")\n    tb_logger = TensorBoardLogger(save_dir=str(output_dir), name=\"tensorboard\")\n\n    # NEW: W&amp;B Logger\n    wb_logger = WandbLogger(\n        project=os.environ.get(\"WANDB_PROJECT\"),\n        entity=os.environ.get(\"WANDB_ENTITY\"),\n        save_dir=str(output_dir),\n        log_model=\"all\",\n        config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True),\n    )\n\n    _ = wb_logger.experiment\n\n    loggers = [wb_logger, csv_logger, tb_logger]\n\n    checkpoint_cb = ModelCheckpoint(\n        dirpath=Path(wb_logger.experiment.dir) / \"checkpoints\",\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_top_k=1,\n        filename=\"epoch{epoch}-val_loss{val_loss:.4f}\",\n        save_on_train_epoch_end=True,\n    )\n\n    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\")\n\n    precision = hparams.get(\"precision\", \"32\")\n\n    # Build profiler from Hydra config\n    profiler_cfg = getattr(cfg, \"profiler\", None)\n    profiler = None\n    if profiler_cfg and profiler_cfg.get(\"type\") != \"none\":\n        if profiler_cfg.type == \"simple\":\n            profiler = SimpleProfiler()\n        elif profiler_cfg.type == \"advanced\":\n            profiler = AdvancedProfiler()\n        elif profiler_cfg.type == \"pytorch\":\n            # Use Hydra output dir for profiler traces\n            profiler_dir = output_dir / \"profiler\"\n            profiler_dir.mkdir(parents=True, exist_ok=True)\n\n            # Handle schedule parameter (can be dict or None)\n            schedule_cfg = profiler_cfg.get(\"schedule\", None)\n            schedule = None\n            if schedule_cfg and isinstance(schedule_cfg, dict):\n                from torch.profiler import schedule as torch_schedule\n\n                schedule = torch_schedule(\n                    wait=int(schedule_cfg.get(\"wait\", 1)),\n                    warmup=int(schedule_cfg.get(\"warmup\", 1)),\n                    active=int(schedule_cfg.get(\"active\", 3)),\n                    repeat=int(schedule_cfg.get(\"repeat\", 2)),\n                )\n\n            # Set up TensorBoard trace if enabled\n            on_trace_ready = None\n            if profiler_cfg.get(\"tensorboard\", False):\n                from torch.profiler import tensorboard_trace_handler\n\n                on_trace_ready = tensorboard_trace_handler(str(profiler_dir))\n\n            profiler = PyTorchProfiler(\n                dirpath=str(profiler_dir),\n                filename=str(profiler_cfg.get(\"filename\", \"trace\")),\n                schedule=schedule,\n                on_trace_ready=on_trace_ready,\n                export_to_trace=bool(profiler_cfg.get(\"export_to_trace\", True)),\n                with_stack=bool(profiler_cfg.get(\"with_stack\", False)),\n                record_module_names=bool(profiler_cfg.get(\"record_module_names\", True)),\n                profile_memory=bool(profiler_cfg.get(\"profile_memory\", True)),\n            )\n\n    trainer = pl.Trainer(\n        max_epochs=int(hparams[\"epochs\"]),\n        precision=precision,\n        accelerator=\"auto\",\n        devices=\"auto\",\n        default_root_dir=str(output_dir),\n        logger=loggers,\n        callbacks=[checkpoint_cb, early_stopping_callback],\n        log_every_n_steps=50,\n        profiler=profiler,\n    )\n\n    logger.info(50 * \"=\")\n    logger.info(\"Starting Lightning training\")\n    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n    logger.info(\"Training complete\")\n\n    checkpoint_dir = Path(wb_logger.experiment.dir) / \"checkpoints\"\n    print(checkpoint_dir)\n\n    # CREATE ARTIFACT\n    if checkpoint_dir.exists() and any(checkpoint_dir.iterdir()):\n        logger.info(f\"Manual Artifact Trigger: Uploading {checkpoint_dir}\")\n        try:\n            # Define the Artifact\n            folder_artifact = wandb.Artifact(name=f\"model-checkpoints-{wb_logger.version}\", type=\"model\")\n            # Add the entire directory to the manifest\n            folder_artifact.add_dir(str(checkpoint_dir))\n            # Log it to the server\n            artifact = wandb.log_artifact(folder_artifact)\n            artifact.wait()  # Wait for upload to complete\n            logger.info(\"Artifact logged successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to log artifact: {e}\")\n    else:\n        logger.warning(\"Checkpoints folder is empty or missing locally.\")\n        # Mandatory: Wait for the upload to hit 100%\n    wandb.finish(quiet=True)\n\n    # Save profiler output to file\n    if profiler:\n        if isinstance(profiler, AdvancedProfiler):\n            profiler_output = profiler.summary()\n            profiler_file = output_dir / \"profiler\" / \"advanced_profiler.txt\"\n            profiler_file.parent.mkdir(parents=True, exist_ok=True)\n            with open(profiler_file, \"w\") as f:\n                f.write(profiler_output)\n            logger.info(f\"Advanced profiler output saved to: {profiler_file}\")\n            logger.info(\"\\n\" + \"=\" * 80)\n            logger.info(\"PROFILER SUMMARY\")\n            logger.info(\"=\" * 80)\n            logger.info(profiler_output)\n        elif isinstance(profiler, PyTorchProfiler):\n            logger.info(f\"Profiler traces saved to: {output_dir / 'profiler'}\")\n            if profiler_cfg is not None and profiler_cfg.get(\"tensorboard\", False):\n                logger.info(f\"View in TensorBoard: tensorboard --logdir={output_dir / 'profiler'}\")\n            else:\n                logger.info(\"View trace.json in Chrome: chrome://tracing\")\n\n    save_path = cfg.dataset.savedTo.path\n    if save_path.startswith(\"gs://\"):\n        # Local save first\n        local_path = \"model.pth\"\n        save(model.state_dict(), local_path)\n        logger.info(f\"Saved model locally to: {local_path}\")\n\n        # Parse bucket and blob\n        # gs://bucket_name/path/to/model.pth\n        try:\n            parts = save_path.replace(\"gs://\", \"\").split(\"/\")\n            bucket_name = parts[0]\n            blob_name = \"/\".join(parts[1:])\n\n            storage_client = storage.Client()\n            bucket = storage_client.bucket(bucket_name)\n            blob = bucket.blob(blob_name)\n            blob.upload_from_filename(local_path)\n            logger.info(f\"Uploaded model to GCS: {save_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to upload model to GCS: {e}\")\n\n        # For compatibility with subsequent logic\n        model_path = Path(local_path)\n    else:\n        model_path = resolve_path(cfg.dataset.savedTo.path)\n        model_path.parent.mkdir(parents=True, exist_ok=True)\n        save(model.state_dict(), str(model_path))\n        logger.info(f\"Saved model to: {model_path}\")\n\n    # Log TensorBoard info\n    tb_log_dir = output_dir / \"tensorboard\"\n    logger.info(f\"TensorBoard logs saved to: {tb_log_dir}\")\n    logger.info(f\"View training metrics: tensorboard --logdir={tb_log_dir}\")\n\n    # Save full composed Hydra config for reproducibility\n    full_cfg_path = output_dir / \"config_full.yaml\"\n    OmegaConf.save(config=cfg, f=str(full_cfg_path))\n    logger.info(f\"Saved full config to: {full_cfg_path}\")\n\n    # Save an artifacts manifest with key file paths\n    artifacts = {\n        \"final_model_state_path\": str(model_path),\n        \"best_checkpoint_path\": str(checkpoint_cb.best_model_path),\n        \"train_log_path\": str(log_path),\n        \"csv_logger_dir\": getattr(csv_logger, \"log_dir\", str(Path(csv_logger.save_dir) / csv_logger.name)),\n        \"hydra_output_dir\": str(output_dir),\n        \"num_workers\": num_workers,\n    }\n    artifacts_path = output_dir / \"artifacts.yaml\"\n    OmegaConf.save(config=OmegaConf.create(artifacts), f=str(artifacts_path))\n    logger.info(f\"Saved artifacts manifest to: {artifacts_path}\")\n</code></pre>"},{"location":"reference/visualize/","title":"<code>Visualize</code>","text":"<p>This module creates a 2D visualization of learned embeddings from a trained model on the CIFAKE test split.</p>"},{"location":"reference/visualize/#what-this-module-does","title":"What this module does","text":"<ul> <li>Loads a <code>FakeArtClassifier</code> checkpoint (<code>state_dict</code>)</li> <li>Loads the processed CIFAKE test tensors from <code>data/processed/</code></li> <li>Computes per-image embeddings from the model (backbone + classifier output)</li> <li>Reduces embeddings to 2D using t-SNE and saves a scatter plot</li> </ul>"},{"location":"reference/visualize/#create-an-embedding-plot","title":"Create an embedding plot","text":"<p>Run via the Invoke task (recommended):</p> <pre><code>uv run invoke visualize\n</code></pre> <p>Or run the module directly:</p> <pre><code>uv run src/fakeartdetector/visualize.py models/model.pth\n</code></pre> <p>You can optionally choose the output filename:</p> <pre><code>uv run src/fakeartdetector/visualize.py models/model.pth --figure-name embeddings.png\n</code></pre> <p>The figure is saved to:</p> <ul> <li><code>reports/figures/&lt;figure_name&gt;</code></li> </ul>"},{"location":"reference/visualize/#notes-and-assumptions","title":"Notes and assumptions","text":"<ul> <li>Expected data files:</li> <li><code>data/processed/test_images.pt</code></li> <li><code>data/processed/test_target.pt</code></li> <li>Device selection: CUDA \u2192 MPS \u2192 CPU</li> <li>Embeddings are taken from <code>model.classifier(model.backbone(images))</code> (before the final 1-unit head)</li> <li>Dimensionality reduction:</li> <li>PCA to 100 dims is only applied if the embedding dimension is greater than 500</li> <li>t-SNE is then applied to produce 2D coordinates</li> </ul>"},{"location":"reference/visualize/#api-reference","title":"API reference","text":""},{"location":"reference/visualize/#fakeartdetector.visualize.visualize","title":"fakeartdetector.visualize.visualize","text":"<pre><code>visualize(model_checkpoint: str = typer.Argument(..., help='Path to a saved model checkpoint (.pth)'), figure_name: str = typer.Option('embeddings.png', help='Output figure file name'), output_dir: str = typer.Option('reports/figures', help='Directory to write the figure into'), data_dir: str = typer.Option('data/processed', help='Directory containing test_images.pt and test_target.pt'), batch_size: int = typer.Option(32, help='Batch size for embedding extraction'), pca_threshold_dim: int = typer.Option(500, help='Apply PCA if embedding dimensionality is above this threshold'), pca_n_components: int = typer.Option(100, help='Number of PCA components (if PCA is applied)'), tsne_perplexity: float = typer.Option(30.0, help='t-SNE perplexity'), tsne_learning_rate: str = typer.Option('auto', help=\"t-SNE learning rate (float as string, or 'auto')\"), seed: int = typer.Option(42, help='Random seed for dimensionality reduction')) -&gt; None\n</code></pre> <p>Visualize model embeddings with t-SNE and save a scatter plot.</p> Source code in <code>src/fakeartdetector/visualize.py</code> <pre><code>@app.command()\ndef visualize(\n    model_checkpoint: str = typer.Argument(..., help=\"Path to a saved model checkpoint (.pth)\"),\n    figure_name: str = typer.Option(\"embeddings.png\", help=\"Output figure file name\"),\n    output_dir: str = typer.Option(\"reports/figures\", help=\"Directory to write the figure into\"),\n    data_dir: str = typer.Option(\"data/processed\", help=\"Directory containing test_images.pt and test_target.pt\"),\n    batch_size: int = typer.Option(32, help=\"Batch size for embedding extraction\"),\n    pca_threshold_dim: int = typer.Option(500, help=\"Apply PCA if embedding dimensionality is above this threshold\"),\n    pca_n_components: int = typer.Option(100, help=\"Number of PCA components (if PCA is applied)\"),\n    tsne_perplexity: float = typer.Option(30.0, help=\"t-SNE perplexity\"),\n    tsne_learning_rate: str = typer.Option(\"auto\", help=\"t-SNE learning rate (float as string, or 'auto')\"),\n    seed: int = typer.Option(42, help=\"Random seed for dimensionality reduction\"),\n) -&gt; None:\n    \"\"\"Visualize model embeddings with t-SNE and save a scatter plot.\"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    model = FakeArtClassifier().to(DEVICE)\n    state_dict = torch.load(model_checkpoint, map_location=DEVICE)\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    processed = Path(data_dir)\n    test_images = torch.load(processed / \"test_images.pt\", map_location=\"cpu\")\n    test_target = torch.load(processed / \"test_target.pt\", map_location=\"cpu\")\n    test_dataset = torch.utils.data.TensorDataset(test_images, test_target)\n    loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n\n    embeddings, targets = [], []\n\n    with torch.inference_mode():\n        for batch in loader:\n            images, target = batch\n            predictions = model.classifier(model.backbone(images.to(DEVICE)))\n            embeddings.append(predictions.cpu())\n            targets.append(target)\n    embeddings = torch.cat(embeddings).numpy()\n    targets = torch.cat(targets).numpy()\n\n    if embeddings.shape[1] &gt; pca_threshold_dim:\n        pca = PCA(n_components=min(pca_n_components, embeddings.shape[1]), random_state=seed)\n        embeddings = pca.fit_transform(embeddings)\n    learning_rate: float | str\n    if tsne_learning_rate.strip().lower() == \"auto\":\n        learning_rate = \"auto\"\n    else:\n        try:\n            learning_rate = float(tsne_learning_rate)\n        except ValueError as exc:\n            raise typer.BadParameter(\"tsne-learning-rate must be a float or 'auto'\") from exc\n\n    tsne = TSNE(\n        n_components=2,\n        perplexity=tsne_perplexity,\n        learning_rate=learning_rate,\n        random_state=seed,\n        init=\"pca\",\n    )\n    embeddings_2d = tsne.fit_transform(embeddings)\n\n    plt.figure(figsize=(10, 10))\n    for i in unique(targets):\n        mask = targets == i\n        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], label=f\"Class {i}\", alpha=0.6)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(output_path / figure_name)\n</code></pre>"}]}